# ğŸ” Databricks â‡„ Snowflake Integration Pipeline

This project demonstrates a robust, bi-directional data integration pipeline between **Databricks** and **Snowflake**, including:

- â« Pushing transformed data from Databricks to Snowflake via **Snowpipe**
- â¬ Pulling Snowflake data into Databricks and writing to **Unity Catalog**
- ğŸ“¦ Organizing data using Delta Lake: `raw` and `silver` layers
- âœ… Ready for enterprise-scale workloads using 100K+ rows

---

## ğŸ“Š Architecture

![Architecture](architecture.png)

---

## ğŸ“ Project Structure

```
Databricks_Snowflake_Integration/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ product_master.csv                  # 100K+ rows (Databricks to Snowflake)
â”‚   â””â”€â”€ inventory_metrics.csv               # 100K+ rows (Snowflake to Databricks)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_push_product_to_snowflake.py     # PySpark â†’ Snowflake external stage
â”‚   â”œâ”€â”€ 02_pull_inventory_from_snowflake.py # Snowflake â†’ Databricks catalog
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ snowpipe_setup.sql                  # Create stage + Snowpipe
â”‚   â””â”€â”€ snowflake_etl_pipeline.sql          # ELT merge into DIM tables
â”œâ”€â”€ catalog/
â”‚   â””â”€â”€ schema_definition.sql               # Table DDLs for Snowflake & Unity Catalog
â”œâ”€â”€ architecture.png                        # Visual pipeline diagram
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ”„ Pipeline Flow

### â¤ **1. Databricks â Snowflake (Push)**

| Step | Description |
|------|-------------|
| ğŸ” Read `product_master.csv` | Transform and clean data using PySpark |
| ğŸ’¾ Write to external stage   | Save as Parquet to S3/Azure Blob (linked to Snowflake stage) |
| ğŸ” Trigger Snowpipe         | Auto-ingest into `PROD_DB.RAW.PRODUCT_STAGE` |
| ğŸ§  Run ELT in Snowflake     | Merge into `PROD_DB.DIM.PRODUCT_DIM` |

---

### â¤ **2. Snowflake â Databricks (Pull)**

| Step | Description |
|------|-------------|
| â„ï¸ Read from Snowflake      | Use Spark connector to fetch `INVENTORY_METRICS` |
| ğŸ§¹ Clean and transform       | Add derived columns, filter, and normalize |
| ğŸ§¾ Save to Unity Catalog     | Write final table to `main.analytics.inventory_metrics_silver` (Delta)

---

## ğŸ“ˆ Datasets

- **`product_master.csv`**: Simulates product dimension (100K+ rows, 12 columns)
- **`inventory_metrics.csv`**: Simulates inventory fact metrics (100K+ rows, 10 columns)

---

## ğŸ› ï¸ Technologies Used

- **Databricks Runtime** (15.4+)
- **Delta Lake**
- **Unity Catalog**
- **Snowflake Snowpipe**
- **Parquet format**
- **S3 / Azure Blob (used both as external stage)**

---

## ğŸ§¾ Output Tables

| Platform   | Database / Catalog | Table                                 | Layer     |
|------------|--------------------|----------------------------------------|-----------|
| Snowflake  | `PROD_DB.RAW`      | `PRODUCT_STAGE`                        | Raw       |
| Snowflake  | `PROD_DB.DIM`      | `PRODUCT_DIM`                          | Final     |
| Databricks | `main.analytics`   | `inventory_metrics_silver`            | Silver    |

---

## ğŸš€ Getting Started

### 1. Upload CSV files:
Upload `product_master.csv` and `inventory_metrics.csv` to `/data/`

### 2. Configure Snowflake Connector in Databricks:

Update `sf_options` in `02_pull_inventory_from_snowflake.py` with your credentials:
```python
sf_options = {
    "sfURL": "your_account.snowflakecomputing.com",
    "sfDatabase": "PROD_DB",
    "sfSchema": "METRICS",
    ...
}
```

### 3. Set up Snowflake Stage + Snowpipe:
Run `snowflake/snowpipe_setup.sql` from Snowflake worksheet

### 4. Run ELT Pipeline:
Execute `snowflake/snowflake_etl_pipeline.sql` in Snowflake to merge raw data
